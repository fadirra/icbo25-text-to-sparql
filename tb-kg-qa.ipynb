{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d3dda-61f4-474d-b14c-86d38b54b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sparqlwrapper langchain langchain-core langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e8fc5-6a5d-4157-a5bf-4321c77ab9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# Advanced libraries\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from utils import entity_link, extract_entity_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d63350-81d8-467a-aa00-115057fea495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector DB set up\n",
    "CHROMA_DB_PATH = \"./wikidata_properties_vector_db\"\n",
    "os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "df = pd.read_csv(\"all-wikidata-properties_v2025.05.27.csv\")  # Must contain: ID, Label, Description\n",
    "\n",
    "df[\"Data Type\"] = df[\"Data Type\"].str.strip()\n",
    "df = df[df[\"Data Type\"].isin([\"Q\", \"WI\"])] # Q = Quantity, WI = WikibaseItem\n",
    "\n",
    "# Create embedding function using SentenceTransformers\n",
    "embedding_fn = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Chroma collection\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"wikidata_properties\",\n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    print(\"Collection is empty. Starting indexing...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        label = row[\"Label\"]\n",
    "        pid = row[\"ID\"]\n",
    "        desc = row.get(\"Description\", \"\")\n",
    "\n",
    "        enriched_text = f\"{label}. {desc}\".strip()\n",
    "\n",
    "        collection.add(\n",
    "            documents=[enriched_text],\n",
    "            metadatas=[{\"id\": pid, \"label\": label, \"description\": desc}],\n",
    "            ids=[str(idx)] # must be unique\n",
    "        )\n",
    "    print(\"Indexing complete.\")\n",
    "    print(f\"Your vector database is now saved at: {os.path.abspath(CHROMA_DB_PATH)}\")\n",
    "else:\n",
    "    print(f\"Collection '{collection.name}' already contains {collection.count()} items.\")\n",
    "    print(f\"Your vector database is located at: {os.path.abspath(CHROMA_DB_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe44538-37a3-4f03-8bd9-ddee1f3ffdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Can add also property description\n",
    "def search_similar_properties(question, collection, top_k=5):\n",
    "    \"\"\"\n",
    "    Given a natural language question and a Chroma collection,\n",
    "    returns top-k similar properties in the format: - `label`: id\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    output = []\n",
    "    for meta in results[\"metadatas\"][0]:\n",
    "        label = meta[\"label\"]\n",
    "        pid = meta[\"id\"]\n",
    "        output.append(f\"- `{label}`: {pid}\")\n",
    "\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5b12e-8d8a-4132-ae92-88ce3d105f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-SPARQL question answering\n",
    "def answer_question(question, llm, qid=None):\n",
    "    \"\"\"\n",
    "    Given a natural language question,\n",
    "    this function extracts the entity, maps it to Wikidata QID, selects the most similar \n",
    "    property using vector search, and constructs a SPARQL query to retrieve the answer.\n",
    "    \n",
    "    It prints the intermediate steps and final SPARQL query result.\n",
    "    \"\"\"\n",
    "\n",
    "    entity_name = extract_entity_property(question, llm)[\"entity\"]\n",
    "    print(f\"# Entity name: {entity_name}\")\n",
    "    \n",
    "    if qid is None:    \n",
    "        qid = entity_link(entity_name)\n",
    "        \n",
    "    print(f\"# Entity QID: {qid}\")\n",
    "\n",
    "    property_mappings = search_similar_properties(question, collection, top_k=3)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "PROMPT TITLE: Ontology-Driven Text-to-SPARQL (Wikidata)\n",
    "\n",
    "ROLE\n",
    "You translate natural-language questions into syntactically valid SPARQL queries for Wikidata, grounded in an explicit ontology (properties/relations) and entity mappings.\n",
    "\n",
    "INPUTS\n",
    "# Natural language question: {question}\n",
    "\n",
    "# QID mapping (Wikidata entity of interest)\n",
    "- `{entity_name}`: {qid}\n",
    "\n",
    "# PID mapping (Wikidata properties or relations)\n",
    "- `has cause`: P828\n",
    "- `disease transmission process`: P1060\n",
    "- `cause of death`: P509\n",
    "- `health specialty`: P1995\n",
    "- `medical examination`: P923\n",
    "- `symptoms and signs`: P780\n",
    "- `commemorates`: P547\n",
    "- `field of work`: P101\n",
    "- `instance of`: P31\n",
    "- `depicts`: P180\n",
    "{property_mappings}\n",
    "\n",
    "OUTPUT CONSTRAINTS\n",
    "- Output ONLY a valid SPARQL query (no prose, no explanations).\n",
    "- Always include the label service:\n",
    "  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "- Prefer concise SELECT variables (?causeLabel, ?symptomLabel, etc.).\n",
    "- The output SPARQL query must be inspired from the query EXAMPLES below, both for the structure, ontological terms, and form.\n",
    "\n",
    "PROCEDURE (high level)\n",
    "- The translation typically relies on entities (QID) and ontological relations or properties (PID) in the questions.\n",
    "- Select an appropriate query pattern: the triple patterns of subject, predicate, and object; directions; and variables.\n",
    "- Compose a minimal SPARQL query with label service; avoid extraneous clauses unless required.\n",
    "- Ensure syntactic validity.\n",
    "\n",
    "EXAMPLES (with ontological terms to reinforce grounding), assume that QIDX is the QID of the entity X.\n",
    "# Natural language: What is the cause of X?\n",
    "   Ontological terms involved: has cause (P828)\n",
    "   SPARQL:\n",
    "   SELECT ?causeLabel WHERE {{\n",
    "     wd:QIDX wdt:P828 ?cause .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: How is X transmitted?\n",
    "   Ontological terms involved: disease transmission process (P1060)\n",
    "   SPARQL:\n",
    "   SELECT ?transmissionLabel WHERE {{\n",
    "     wd:QIDX wdt:P1060 ?transmission .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: What is the cause of death of X?\n",
    "   Ontological terms involved: cause of death (P509)\n",
    "   SPARQL:\n",
    "   SELECT ?causeLabel WHERE {{\n",
    "     wd:QIDX wdt:P509 ?cause .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: What medical specialties are involved in treating X?\n",
    "   Ontological terms involved: health specialty (P1995)\n",
    "   SPARQL:\n",
    "   SELECT ?specialtyLabel WHERE {{\n",
    "     wd:QIDX wdt:P1995 ?specialty .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: X is a medical exam for which disease?\n",
    "   Ontological terms involved: medical examination (P923)\n",
    "   SPARQL:\n",
    "   SELECT ?diseaseLabel WHERE {{\n",
    "     ?disease wdt:P923 wd:QIDX .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: What are the symptoms of X?\n",
    "   Ontological terms involved: symptoms and signs (P780)\n",
    "   SPARQL:\n",
    "   SELECT ?symptomLabel WHERE {{\n",
    "     wd:QIDX wdt:P780 ?symptom .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: What event commemorates X?\n",
    "   Ontological terms involved: commemorates (P547)\n",
    "   SPARQL:\n",
    "   SELECT ?eventLabel WHERE {{\n",
    "     ?event wdt:P547 wd:QIDX .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: Who are some researchers working in the field of X?\n",
    "   Ontological terms involved: field of work (P101)\n",
    "   SPARQL:\n",
    "   SELECT ?researcherLabel WHERE {{\n",
    "     ?researcher wdt:P101 wd:QIDX .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: X corresponds to what types?\n",
    "   Ontological terms involved: instance of (P31)\n",
    "   SPARQL:\n",
    "   SELECT ?typeLabel WHERE {{\n",
    "     wd:QIDX wdt:P31 ?type .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "# Natural language: The painting of X depicts what disease?\n",
    "   Ontological terms involved: depicts (P180)\n",
    "   SPARQL:\n",
    "   SELECT ?depictionLabel WHERE {{\n",
    "     wd:QIDX wdt:P180 ?depiction .\n",
    "     SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "   }}\n",
    "\n",
    "FINAL INSTRUCTION\n",
    "Now produce ONLY the SPARQL query for:\n",
    "Q: {question}\n",
    "A:\n",
    "\"\"\")\n",
    "\n",
    "    # Ground the prompt with actual parameters\n",
    "    grounded_prompt = prompt.format(\n",
    "        question=question,\n",
    "        entity_name=entity_name,\n",
    "        qid=qid,\n",
    "        property_mappings=property_mappings\n",
    "    )\n",
    "    # print(\"# Grounded Prompt:\\n\" + grounded_prompt)\n",
    "    \n",
    "    chain = prompt | llm\n",
    "\n",
    "    sparql_query = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"entity_name\": entity_name,\n",
    "        \"qid\": qid,\n",
    "        \"property_mappings\": property_mappings\n",
    "    }).strip()\n",
    "\n",
    "    print(\"# SPARQL query string:\\n\" + sparql_query)\n",
    "\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(sparql_query)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyWikidataApp/1.0 (mywikidata@example.cm)\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    # print(\"# SPARQL Query Result:\")\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        answers = []\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            for key, val in result.items():\n",
    "                answers.append(val[\"value\"].strip())\n",
    "        # print(answers if answers else \"[]\")\n",
    "        return answers\n",
    "    except Exception as e:\n",
    "        print(\"SPARQL query failed:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a01be-d978-4fcb-9013-8c12e872299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive question answering\n",
    "def answer_question_naive(question, llm, qid=None):\n",
    "\n",
    "    entity_name = extract_entity_property(question, llm)[\"entity\"]\n",
    "    print(f\"# Entity name: {entity_name}\")\n",
    "    \n",
    "    if qid is None:    \n",
    "        qid = entity_link(entity_name)\n",
    "        \n",
    "    print(f\"# Entity QID: {qid}\")\n",
    "\n",
    "    property_mappings = search_similar_properties(question, collection, top_k=3)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "PROMPT TITLE: Text-to-SPARQL (Wikidata)\n",
    "\n",
    "ROLE\n",
    "You translate natural-language questions into syntactically valid SPARQL queries for Wikidata.\n",
    "\n",
    "INPUTS\n",
    "# Natural language question: {question}\n",
    "\n",
    "# QID mapping (Wikidata entity of interest)\n",
    "- `{entity_name}`: {qid}\n",
    "\n",
    "OUTPUT CONSTRAINTS\n",
    "- Output ONLY a valid SPARQL query (no prose, no explanations).\n",
    "- Always include the label service:\n",
    "  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" }}\n",
    "- Prefer concise SELECT variables (?causeLabel, ?symptomLabel, etc.).\n",
    "\n",
    "PROCEDURE (high level)\n",
    "- The translation typically relies on entities (QID) in the questions.\n",
    "- Select an appropriate query pattern: the triple patterns of subject, predicate, and object; directions; and variables.\n",
    "- Compose a minimal SPARQL query with label service; avoid extraneous clauses unless required.\n",
    "- Ensure syntactic validity.\n",
    "\n",
    "FINAL INSTRUCTION\n",
    "Now produce ONLY the SPARQL query for:\n",
    "Q: {question}\n",
    "A:\n",
    "\"\"\")\n",
    "\n",
    "    # Ground the prompt with actual parameters\n",
    "    grounded_prompt = prompt.format(\n",
    "        question=question,\n",
    "        entity_name=entity_name,\n",
    "        qid=qid,\n",
    "        property_mappings=property_mappings\n",
    "    )\n",
    "    print(\"# Grounded Prompt:\\n\" + grounded_prompt)\n",
    "    \n",
    "    chain = prompt | llm\n",
    "\n",
    "    sparql_query = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"entity_name\": entity_name,\n",
    "        \"qid\": qid,\n",
    "        \"property_mappings\": property_mappings\n",
    "    }).strip()\n",
    "\n",
    "    print(\"# SPARQL query string:\\n\" + sparql_query)\n",
    "\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(sparql_query)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"MyWikidataApp/1.0 (mywikidata@example.cm)\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    # print(\"# SPARQL Query Result:\")\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        answers = []\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            for key, val in result.items():\n",
    "                answers.append(val[\"value\"].strip())\n",
    "        # print(answers if answers else \"[]\")\n",
    "        return answers\n",
    "    except Exception as e:\n",
    "        print(\"SPARQL query failed:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49e8a2-59a2-43c0-916e-99aadcf6c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive text-to-SPARQL testing\n",
    "\n",
    "question = \"Can a chest radiograph be used for the medical examination of tuberculosis?\"\n",
    "question = \"What is the cause of death of General Soedirman?\"\n",
    "question = \"Researchers working in the field of tuberculosis?\"\n",
    "question = \"What causes tuberculosis?\"\n",
    "\n",
    "llm = OllamaLLM(model=\"gpt-oss:20b\", reasoning=False)\n",
    "llm = OllamaLLM(model=\"mistral:7b\", reasoning=False)\n",
    "llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "answer_question_naive(question, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cea97-8cb8-4d24-a0c4-d4026ee9af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-SPARQL testing\n",
    "\n",
    "question = \"Can a chest radiograph be used for the medical examination of tuberculosis?\"\n",
    "question = \"What is the cause of death of General Soedirman?\"\n",
    "question = \"Researchers working in the field of tuberculosis?\"\n",
    "question = \"What causes tuberculosis?\"\n",
    "\n",
    "llm = OllamaLLM(model=\"gpt-oss:20b\", reasoning=False)\n",
    "llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "llm = OllamaLLM(model=\"mistral:7b\", reasoning=False)\n",
    "\n",
    "answer_question(question, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352923e-6bca-4c0b-aad6-7422afb0f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_list(x):\n",
    "    \"\"\"Accept list or comma-separated string with quotes -> list[str].\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    \n",
    "    # Use csv.reader to handle quoted fields with commas\n",
    "    reader = csv.reader(io.StringIO(str(x)), skipinitialspace=True)\n",
    "    parts = next(reader)  # single row\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip surrounding quotes/whitespace, collapse inner spaces.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().strip('\"').strip(\"'\").lower()\n",
    "    # normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def to_norm_set(items):\n",
    "    \"\"\"items (list[str]) -> normalized set[str], dropping empties.\"\"\"\n",
    "    return {normalize_item(i) for i in items if normalize_item(i)}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    \"\"\"|A ∩ B| / |A ∪ B|; define as 1.0 when both empty.\"\"\"\n",
    "    union = a | b\n",
    "    inter = a & b\n",
    "    if len(union) == 0:\n",
    "        return 1.0\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "df = pd.read_csv(\"question-dataset.csv\", \n",
    "                 quotechar='\"', \n",
    "                 engine=\"python\")\n",
    "df[\"NL Groundings for TB\"] = df[\"NL Groundings for TB\"].apply(ensure_list)\n",
    "df[\"Results (2025.09.14)\"] = df[\"Results (2025.09.14)\"].apply(ensure_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34bf18-f457-4e02-be6b-31a104122ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, llm):\n",
    "\tall_scores = []\n",
    "\tfor idx, row in df.iterrows():\n",
    "\t\tnl_group = row[\"Natural Language\"]\n",
    "\t\tnl_variations = row[\"NL Groundings for TB\"]\n",
    "\t\tgt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "\t\tprint(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "\t\tfor nl_q in nl_variations:\n",
    "\t\t\tprint(f\"Question: {nl_q}\")\n",
    "\t\t\tquery_eval_results = answer_question(nl_q, llm)\n",
    "\t\t\tprint(query_eval_results)\n",
    "\n",
    "\t\t\tpred_set = to_norm_set(query_eval_results)\n",
    "\t\t\tscore = jaccard(pred_set, gt_results_set)\n",
    "\t\t\tall_scores.append(score)\n",
    "\n",
    "\t\t\tprint(f\"- NL: {nl_q}\")\n",
    "\t\t\tprint(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "\t\t\tprint(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "\t\t\tprint(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "\toverall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "\tprint(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a04c3-664d-4454-ba9a-d51a19bb4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_naive(df, llm):\n",
    "\tall_scores = []\n",
    "\tfor idx, row in df.iterrows():\n",
    "\t\tnl_group = row[\"Natural Language\"]\n",
    "\t\tnl_variations = row[\"NL Groundings for TB\"]\n",
    "\t\tgt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "\t\tprint(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "\t\tfor nl_q in nl_variations:\n",
    "\t\t\tprint(f\"Question: {nl_q}\")\n",
    "\t\t\tquery_eval_results = answer_question_naive(nl_q, llm)\n",
    "\t\t\tprint(query_eval_results)\n",
    "\n",
    "\t\t\tpred_set = to_norm_set(query_eval_results)\n",
    "\t\t\tscore = jaccard(pred_set, gt_results_set)\n",
    "\t\t\tall_scores.append(score)\n",
    "\n",
    "\t\t\tprint(f\"- NL: {nl_q}\")\n",
    "\t\t\tprint(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "\t\t\tprint(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "\t\t\tprint(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "\toverall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "\tprint(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618d69a-58e0-40db-8eff-7e1e32367a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"gemma3:12b\", reasoning=False)\n",
    "\n",
    "run_experiment(df, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43be68-0f82-497c-b0e7-791582968336",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"gpt-oss:20b\", reasoning=False)\n",
    "\n",
    "run_experiment(df, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26820c2e-fee8-4453-a020-9cf65773ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1:8b\", reasoning=False)\n",
    "\n",
    "run_experiment(df, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82e8bd-4d17-4ee1-8a6f-913d1c307f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"mistral:7b\", reasoning=False)\n",
    "\n",
    "run_experiment(df, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163800e-fe31-441d-bc14-95d129829096",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "run_experiment(df, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58777556-7b20-45c6-ba93-8e7d4fe3224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "run_experiment_naive(df, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aaa9f5-953d-4d0a-88a8-74f12996eb59",
   "metadata": {},
   "source": [
    "# BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d6d9a-6614-44ea-aa3e-50be18c8e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main loop -------------------------------------------------------------\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    nl_group = row[\"Natural Language\"]\n",
    "    nl_variations = row[\"NL Groundings for TB\"]\n",
    "    gt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "    print(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "    for nl_q in nl_variations:\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Replace the next line with YOUR query evaluation for `nl_q`.\n",
    "        # It must produce an iterable of answers (strings).\n",
    "        #\n",
    "        # Example expected type:\n",
    "        #   query_eval_results = [\"mycobacterium tuberculosis\"]\n",
    "        #\n",
    "        # >>> BEGIN YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        print(f\"Question: {nl_q}\")\n",
    "        query_eval_results = answer_question(nl_q, llm)\n",
    "        print(query_eval_results)\n",
    "        # >>> END YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        pred_set = to_norm_set(query_eval_results)\n",
    "        score = jaccard(pred_set, gt_results_set)\n",
    "        all_scores.append(score)\n",
    "\n",
    "        print(f\"- NL: {nl_q}\")\n",
    "        print(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "        print(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "        print(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "# --- Overall ---------------------------------------------------------------\n",
    "\n",
    "overall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "print(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a28670-32a8-47a4-a217-91c275f45892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# --- Helpers ---------------------------------------------------------------\n",
    "\n",
    "def ensure_list(x):\n",
    "    \"\"\"Accept list or comma-separated string with quotes -> list[str].\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    \n",
    "    # Use csv.reader to handle quoted fields with commas\n",
    "    reader = csv.reader(io.StringIO(str(x)), skipinitialspace=True)\n",
    "    parts = next(reader)  # single row\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip surrounding quotes/whitespace, collapse inner spaces.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().strip('\"').strip(\"'\").lower()\n",
    "    # normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def to_norm_set(items):\n",
    "    \"\"\"items (list[str]) -> normalized set[str], dropping empties.\"\"\"\n",
    "    return {normalize_item(i) for i in items if normalize_item(i)}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    \"\"\"|A ∩ B| / |A ∪ B|; define as 1.0 when both empty.\"\"\"\n",
    "    union = a | b\n",
    "    inter = a & b\n",
    "    if len(union) == 0:\n",
    "        return 1.0\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "# --- Preprocess columns ----------------------------------------------------\n",
    "\n",
    "# Make sure these columns exist in your df\n",
    "# df = ...  # your existing DataFrame\n",
    "\n",
    "# Ensure \"NL Groundings for TB\" is a list per row\n",
    "df[\"NL Groundings for TB\"] = df[\"NL Groundings for TB\"].apply(ensure_list)\n",
    "\n",
    "# Ensure ground-truth results are a list\n",
    "df[\"Results (2025.09.14)\"] = df[\"Results (2025.09.14)\"].apply(ensure_list)\n",
    "\n",
    "# --- Main loop -------------------------------------------------------------\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False) # not good, all false\n",
    "# llm = OllamaLLM(model=\"mistral:7b\", reasoning=False)\n",
    "llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    nl_group = row[\"Natural Language\"]\n",
    "    nl_variations = row[\"NL Groundings for TB\"]\n",
    "    gt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "    print(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "    for nl_q in nl_variations:\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Replace the next line with YOUR query evaluation for `nl_q`.\n",
    "        # It must produce an iterable of answers (strings).\n",
    "        #\n",
    "        # Example expected type:\n",
    "        #   query_eval_results = [\"mycobacterium tuberculosis\"]\n",
    "        #\n",
    "        # >>> BEGIN YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        print(f\"Question: {nl_q}\")\n",
    "        query_eval_results = answer_question(nl_q, llm)\n",
    "        print(query_eval_results)\n",
    "        # >>> END YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        pred_set = to_norm_set(query_eval_results)\n",
    "        score = jaccard(pred_set, gt_results_set)\n",
    "        all_scores.append(score)\n",
    "\n",
    "        print(f\"- NL: {nl_q}\")\n",
    "        print(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "        print(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "        print(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "# --- Overall ---------------------------------------------------------------\n",
    "\n",
    "overall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "print(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440269a-8dfa-4a79-8250-3b95160e617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# --- Helpers ---------------------------------------------------------------\n",
    "\n",
    "def ensure_list(x):\n",
    "    \"\"\"Accept list or comma-separated string with quotes -> list[str].\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    \n",
    "    # Use csv.reader to handle quoted fields with commas\n",
    "    reader = csv.reader(io.StringIO(str(x)), skipinitialspace=True)\n",
    "    parts = next(reader)  # single row\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip surrounding quotes/whitespace, collapse inner spaces.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().strip('\"').strip(\"'\").lower()\n",
    "    # normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def to_norm_set(items):\n",
    "    \"\"\"items (list[str]) -> normalized set[str], dropping empties.\"\"\"\n",
    "    return {normalize_item(i) for i in items if normalize_item(i)}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    \"\"\"|A ∩ B| / |A ∪ B|; define as 1.0 when both empty.\"\"\"\n",
    "    union = a | b\n",
    "    inter = a & b\n",
    "    if len(union) == 0:\n",
    "        return 1.0\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "# --- Preprocess columns ----------------------------------------------------\n",
    "\n",
    "# Make sure these columns exist in your df\n",
    "# df = ...  # your existing DataFrame\n",
    "\n",
    "# Ensure \"NL Groundings for TB\" is a list per row\n",
    "df[\"NL Groundings for TB\"] = df[\"NL Groundings for TB\"].apply(ensure_list)\n",
    "\n",
    "# Ensure ground-truth results are a list\n",
    "df[\"Results (2025.09.14)\"] = df[\"Results (2025.09.14)\"].apply(ensure_list)\n",
    "\n",
    "# --- Main loop -------------------------------------------------------------\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False) # not good, all false\n",
    "llm = OllamaLLM(model=\"mistral:7b\", reasoning=False)\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    nl_group = row[\"Natural Language\"]\n",
    "    nl_variations = row[\"NL Groundings for TB\"]\n",
    "    gt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "    print(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "    for nl_q in nl_variations:\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Replace the next line with YOUR query evaluation for `nl_q`.\n",
    "        # It must produce an iterable of answers (strings).\n",
    "        #\n",
    "        # Example expected type:\n",
    "        #   query_eval_results = [\"mycobacterium tuberculosis\"]\n",
    "        #\n",
    "        # >>> BEGIN YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        print(f\"Question: {nl_q}\")\n",
    "        query_eval_results = answer_question(nl_q, llm)\n",
    "        print(query_eval_results)\n",
    "        # >>> END YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        pred_set = to_norm_set(query_eval_results)\n",
    "        score = jaccard(pred_set, gt_results_set)\n",
    "        all_scores.append(score)\n",
    "\n",
    "        print(f\"- NL: {nl_q}\")\n",
    "        print(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "        print(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "        print(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "# --- Overall ---------------------------------------------------------------\n",
    "\n",
    "overall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "print(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1927ce9-f357-452b-86b1-1a5d47f41749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# --- Helpers ---------------------------------------------------------------\n",
    "\n",
    "def ensure_list(x):\n",
    "    \"\"\"Accept list or comma-separated string with quotes -> list[str].\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    \n",
    "    # Use csv.reader to handle quoted fields with commas\n",
    "    reader = csv.reader(io.StringIO(str(x)), skipinitialspace=True)\n",
    "    parts = next(reader)  # single row\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def normalize_item(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip surrounding quotes/whitespace, collapse inner spaces.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().strip('\"').strip(\"'\").lower()\n",
    "    # normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def to_norm_set(items):\n",
    "    \"\"\"items (list[str]) -> normalized set[str], dropping empties.\"\"\"\n",
    "    return {normalize_item(i) for i in items if normalize_item(i)}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    \"\"\"|A ∩ B| / |A ∪ B|; define as 1.0 when both empty.\"\"\"\n",
    "    union = a | b\n",
    "    inter = a & b\n",
    "    if len(union) == 0:\n",
    "        return 1.0\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "# --- Preprocess columns ----------------------------------------------------\n",
    "\n",
    "# Make sure these columns exist in your df\n",
    "# df = ...  # your existing DataFrame\n",
    "\n",
    "# Ensure \"NL Groundings for TB\" is a list per row\n",
    "df[\"NL Groundings for TB\"] = df[\"NL Groundings for TB\"].apply(ensure_list)\n",
    "\n",
    "# Ensure ground-truth results are a list\n",
    "df[\"Results (2025.09.14)\"] = df[\"Results (2025.09.14)\"].apply(ensure_list)\n",
    "\n",
    "# --- Main loop -------------------------------------------------------------\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False) # not good, all false\n",
    "llm = OllamaLLM(model=\"gemma3:12b\", reasoning=False)\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    nl_group = row[\"Natural Language\"]\n",
    "    nl_variations = row[\"NL Groundings for TB\"]\n",
    "    gt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "    print(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "    for nl_q in nl_variations:\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Replace the next line with YOUR query evaluation for `nl_q`.\n",
    "        # It must produce an iterable of answers (strings).\n",
    "        #\n",
    "        # Example expected type:\n",
    "        #   query_eval_results = [\"mycobacterium tuberculosis\"]\n",
    "        #\n",
    "        # >>> BEGIN YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        print(f\"Question: {nl_q}\")\n",
    "        query_eval_results = answer_question(nl_q, llm)\n",
    "        print(query_eval_results)\n",
    "        # >>> END YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        pred_set = to_norm_set(query_eval_results)\n",
    "        score = jaccard(pred_set, gt_results_set)\n",
    "        all_scores.append(score)\n",
    "\n",
    "        print(f\"- NL: {nl_q}\")\n",
    "        print(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "        print(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "        print(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "# --- Overall ---------------------------------------------------------------\n",
    "\n",
    "overall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "print(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be760c8-e40c-4626-9ed2-4e25682ac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Preprocess columns ----------------------------------------------------\n",
    "\n",
    "# Make sure these columns exist in your df\n",
    "# df = ...  # your existing DataFrame\n",
    "\n",
    "# Ensure \"NL Groundings for TB\" is a list per row\n",
    "df[\"NL Groundings for TB\"] = df[\"NL Groundings for TB\"].apply(ensure_list)\n",
    "\n",
    "# Ensure ground-truth results are a list\n",
    "df[\"Results (2025.09.14)\"] = df[\"Results (2025.09.14)\"].apply(ensure_list)\n",
    "\n",
    "# --- Main loop -------------------------------------------------------------\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False) # not good, all false\n",
    "llm = OllamaLLM(model=\"llama3.1:8b\", reasoning=False)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    nl_group = row[\"Natural Language\"]\n",
    "    nl_variations = row[\"NL Groundings for TB\"]\n",
    "    gt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "    print(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "    for nl_q in nl_variations:\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Replace the next line with YOUR query evaluation for `nl_q`.\n",
    "        # It must produce an iterable of answers (strings).\n",
    "        #\n",
    "        # Example expected type:\n",
    "        #   query_eval_results = [\"mycobacterium tuberculosis\"]\n",
    "        #\n",
    "        # >>> BEGIN YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        print(f\"Question: {nl_q}\")\n",
    "        query_eval_results = answer_question(nl_q, llm)\n",
    "        print(query_eval_results)\n",
    "        # >>> END YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        pred_set = to_norm_set(query_eval_results)\n",
    "        score = jaccard(pred_set, gt_results_set)\n",
    "        all_scores.append(score)\n",
    "\n",
    "        print(f\"- NL: {nl_q}\")\n",
    "        print(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "        print(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "        print(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "# --- Overall ---------------------------------------------------------------\n",
    "\n",
    "overall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "print(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82003a-b188-4b34-bbed-f1aa9ba00823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Preprocess columns ----------------------------------------------------\n",
    "\n",
    "# Make sure these columns exist in your df\n",
    "# df = ...  # your existing DataFrame\n",
    "\n",
    "# Ensure \"NL Groundings for TB\" is a list per row\n",
    "df[\"NL Groundings for TB\"] = df[\"NL Groundings for TB\"].apply(ensure_list)\n",
    "\n",
    "# Ensure ground-truth results are a list\n",
    "df[\"Results (2025.09.14)\"] = df[\"Results (2025.09.14)\"].apply(ensure_list)\n",
    "\n",
    "# --- Main loop -------------------------------------------------------------\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "# llm = OllamaLLM(model=\"llama3.2\", reasoning=False) # not good, all false\n",
    "llm = OllamaLLM(model=\"qwen3:8b\", reasoning=False)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    nl_group = row[\"Natural Language\"]\n",
    "    nl_variations = row[\"NL Groundings for TB\"]\n",
    "    gt_results_set = to_norm_set(row[\"Results (2025.09.14)\"])\n",
    "\n",
    "    print(f\"\\n### NL Group: {nl_group}\")\n",
    "\n",
    "    for nl_q in nl_variations:\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Replace the next line with YOUR query evaluation for `nl_q`.\n",
    "        # It must produce an iterable of answers (strings).\n",
    "        #\n",
    "        # Example expected type:\n",
    "        #   query_eval_results = [\"mycobacterium tuberculosis\"]\n",
    "        #\n",
    "        # >>> BEGIN YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        print(f\"Question: {nl_q}\")\n",
    "        query_eval_results = answer_question(nl_q, llm)\n",
    "        print(query_eval_results)\n",
    "        # >>> END YOUR EVALUATION CODE FOR `nl_q` <<<\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        pred_set = to_norm_set(query_eval_results)\n",
    "        score = jaccard(pred_set, gt_results_set)\n",
    "        all_scores.append(score)\n",
    "\n",
    "        print(f\"- NL: {nl_q}\")\n",
    "        print(f\"  Pred: {sorted(pred_set) if pred_set else '[]'}\")\n",
    "        print(f\"  Gold: {sorted(gt_results_set) if gt_results_set else '[]'}\")\n",
    "        print(f\"  Jaccard: {score:.4f}\")\n",
    "\n",
    "# --- Overall ---------------------------------------------------------------\n",
    "\n",
    "overall = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "print(f\"\\n=== Overall average Jaccard: {overall:.4f} ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
